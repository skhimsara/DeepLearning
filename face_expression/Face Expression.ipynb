{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"/Users/sidnpoo/Downloads/MLStuff_DoNotDelete/_DATASETS_/face_expresson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emotion                                             pixels     Usage\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
      "5        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
      "6        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
      "7        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
      "8        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training\n",
      "9        2  255 254 255 254 254 179 122 107 95 124 149 150...  Training\n"
     ]
    }
   ],
   "source": [
    "#Read the csv as dataframe to understand what's in there.\n",
    "df = pd.read_csv(DATA_PATH+ \"/fer2013.csv\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PrivateTest     3589\n",
       "PublicTest      3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of examples\n",
    "print(len(df))\n",
    "#find uniques in Usage column and how many\n",
    "df.Usage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8989\n",
       "6    6198\n",
       "4    6077\n",
       "2    5121\n",
       "0    4953\n",
       "5    4002\n",
       "1     547\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique classes\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expressions = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
    "expressions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, entry_type):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    with open(filename , 'r') as f:\n",
    "        f.readline() #skip first line\n",
    "        for line in f:\n",
    "                if str(entry_type) in line:\n",
    "                \n",
    "                    row = line.split(',')\n",
    "                    y.append(int(row[0]))\n",
    "                    X.append([int(p) for p in row[1].split()] )\n",
    "           \n",
    "            \n",
    "            \n",
    "    return np.array(X),np.array(y)\n",
    "          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(entry_type):\n",
    "    X_file  = entry_type+\"_X.npy\"\n",
    "    y_file = entry_type+\"_y.npy\"\n",
    "    \n",
    "    if os.path.exists(X_file) and os.path.exists(y_file):\n",
    "        print(\"Found numpy arrays saved a files..loading \")\n",
    "        X,y = np.load(X_file), np.load(y_file)\n",
    "    else:\n",
    "        print(\"Numpy array file missing..\")\n",
    "        X, y = read_data(DATA_PATH+ \"/fer2013.csv\", str(entry_type))\n",
    "        #save np array to file for faster processing for 2nd run\n",
    "\n",
    "        np.save(X_file, X)\n",
    "        np.save(y_file,y)\n",
    "        \n",
    "        #normalize the X before returning it\n",
    "    X = X.astype(np.float32) / 255.0\n",
    "    y = y.astype(np.int32)   \n",
    "    return X , y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27450982, 0.3137255 , 0.32156864, ..., 0.41568628, 0.42745098,\n",
       "        0.32156864],\n",
       "       [0.5921569 , 0.5882353 , 0.5764706 , ..., 0.75686276, 0.7176471 ,\n",
       "        0.72156864],\n",
       "       [0.90588236, 0.83137256, 0.6117647 , ..., 0.34509805, 0.43137255,\n",
       "        0.59607846],\n",
       "       ...,\n",
       "       [0.2901961 , 0.31764707, 0.34117648, ..., 0.7372549 , 0.73333335,\n",
       "        0.73333335],\n",
       "       [0.87058824, 0.8901961 , 0.79607844, ..., 0.53333336, 0.53333336,\n",
       "        0.5254902 ],\n",
       "       [0.7647059 , 0.78039217, 0.8039216 , ..., 0.02352941, 0.05882353,\n",
       "        0.14901961]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numpy arrays saved a files..loading \n",
      "Found numpy arrays saved a files..loading \n",
      "Found numpy arrays saved a files..loading \n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = load_data(\"Training\")\n",
    "\n",
    "X_val,y_val = load_data(\"PrivateTest\")\n",
    "\n",
    "X_test,y_test = load_data(\"PublicTest\")\n",
    "\n",
    "\n",
    "#reshape to 48x48 and add a dimension\n",
    "X_train = X_train.reshape(X_train.shape[0],IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "X_val = X_val.reshape(X_val.shape[0],IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],IMAGE_SIZE,IMAGE_SIZE,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXusntV15p/lg8EEggn4gu1jfMcXjA3BiSCZSNwSJbQC/iBS02pEJSL+aaVUbdWQVhpNpRkp+afpHzPqCE2ieqQK0pZKRFGjkWVMKkQCNokBG4NtfL9gQ8DgBDAY7/njfI78Pvs5/hbH9ufj2c9Pss7Z2/t93/1e1nm/9XxrrR2lFBhj2mLC+Z6AMWbw2PCNaRAbvjENYsM3pkFs+MY0iA3fmAax4RvTIDZ8YxrkjAw/Ir4aEa9GxPaIePhsTcoYc26JsUbuRcQQgK0AvgxgH4D1AL5RSnl5tG0uuuiiMnHixE7fJZdc0ml/+tOfrrb78MMPO+3333+/GjN16tRO+ze/+U01ho914sSJaszQ0NBpjw0Al156aac9adKkasyECfXfVN73Bx98UI2JiNNuM9q+zyfqOmaeKx6jtuF9q2N99NFHfccw/CwAwHvvvVf1vfvuu532pz71qWrMxx9/3Gmr+8r38fjx49WYsVwzfl4++ugjHD9+vNspuKjvkUbn8wC2l1J29CbwGIB7AYxq+BMnTsT8+fM7fQsWLOi0b7/99mq7vXv3dtqbNm2qxnzzm9/stH/xi19UY+bOndtpqz8gV1xxRae9f//+asyyZcs67SVLllRj1B+Dq666qtPesmVLNYYfEN5G7Vv9IeCHX43hh0bBDzVQP3zHjh2rxvCDrR5qHqP2w8aojPPQoUOd9m9/+9tqDB9/3rx51ZiNGzdWfWvXru20V65cWY3hPw7qvn7mM5/ptN98881qDF/rzB8Hvoe7du2qtlGcyatjFoBTLXJfr88YM845kze+el1Uf9Yj4iEADwEjb3xjzPnnTN74+wDMPqU9DOAADyqlPFJKWVVKWaX8VWPM4DmTN/56AIsiYh6A/QD+AMAfnm6Diy++GMPDw52+6dOnn7YNAM8880ynffnll1dj2F8/evSoPP6psG+mUH4Wn4Mac+WVV1Z9Bw8e7LSV/zxlypROW4lJ/AdU/UHlOakxGR9fwfPO/EFX58rb8f1R2ynhbvLkyZ22uve8HyX+sgYE1NqI0g/4eEpP4fuRufbquqrrOBbGbPillOMR8acA/i+AIQA/LKVsPiuzMsacU87kjY9Syr8D+PezNBdjzIAYX18IG2MGwhm98T8pEydOxKxZ3W/82I9RPgx/53nTTTf1HcMBPUDtn6nv2nk/7M8DdZCR+rZCBXHwvpUOcNFF3VuSCQTibYD6+141R973WIO5xrpdJhCJ562OxZrPZZddVo359a9/3WkrfUfpS6y5qNgPfq7U/eBz5aAjoPb71bnyfjLxEgq/8Y1pEBu+MQ1iwzemQWz4xjTIQMW9EydOVEIIC1wqKYbFLBXUwuJNZsw111xTjXnnnXc6bZWAwxl7KqNw3759VR8HqChxMSPOsMCjAj1UkFO/Y6ljK6GKBVg1JiNUMZlsSXUsvq4c0APU914lBCn4GdmxY0c1hu8/P0NA7lrzNcskX401CMtvfGMaxIZvTIPY8I1pkIH6+B9++CH27NnT6ePCHFu3bq224yAa5Ru/9dZbnbZKwmD/UAVxzJgxo9NWgS/cp4I61PE5qCjjnym/l31srgiUJXN8NYbPP5M4ovbDPqw6Vx6j7kcm2YfvvUqsUkU+rr322k5bFYHJ6BAcsJNJ5FH7yVQXyuA3vjENYsM3pkFs+MY0iA3fmAYZqLhXSqkEjEylGBbzlAijBDaGAztYEASAFStWdNoqi4oFptdff70ao8o3K9GJUYIOwwKPCkbJBMzwfNSxVZYhowQ3FvwyVX4zQS2Zqr/qWBzQpcqmq3vNgqwKDuJrpLIu+ZlVzznPKfMsqDln8BvfmAax4RvTIDZ8YxpkoD7+pZdeiuuvv747gYQfwz7+22+/XY1hP1etQMM6gBrD1VuUL8b+mvKzeOWULBnfOLMcE4/JBIxkAmjUvtX5Z5a+Yt8846+q/fB5qDnzfVU6kbqOXIHn6quvrsbs3r2701YJUvx8qqCrsSTc8DbZffiNb0yD2PCNaRAbvjENYsM3pkEGKu5dcsklVTYeCzpKcOPqKWqJJBZ0VFALC2c333xzNYYFJyXCcIUVXlob0AE8YylDrYQqDphRx+IxSvTh66HEtUzAjCKTRcb7VnPM7CdTppuDldS5ZrI158yZU405cKC7ZKQ6D67SowLOWMRWGZ79BFmX1zbGjIoN35gGseEb0yAD9fGB2h9j30stQ3z48OFOW/l9ys9meIkkFWTD/llGK1AVgTJ+pyKzLHRmma1MNZfM8tbKX2X/WO2bNQY1JlPpVgXVMPwMqW0ygWIqIYnvx8KFC6sxL730UqetnmHWCg4dOlSNYf9cJXVlqvVm8BvfmAax4RvTIDZ8YxrEhm9Mgwxc3GNYBNq1a1c1hgN2VMUXJbAxs2bN6rRVEAcH7LzxxhvVGM7YypR8zpLJzmOUSMYCl5pjRiRUATws+GWEs4xIlwngGWtATyZYSAXVsFA3e/bsagz3Pf/889UYFuGmTZtWjVHLxzE8b37OnJ1njBkVG74xDdLX8CPihxFxOCI2ndJ3VUSsiYhtvZ9jqzphjDkvZHz8fwTwPwD8n1P6HgawtpTy3Yh4uNf+dr8dRUTlD3JwzpEjR6rt2I9R/jz3qWWyuU/th4+fSaZQ/rxKnGH/VGkMmX3zftQY9nOVj51Z+ipDZnkspQNkKufwualzzZwHXzM1H5X8xc/D8uXLqzEcnMPVfoC6ErNaop2T0VTSUEYrydD3jV9K+Q8AXIf6XgCre7+vBnDfWZmNMWYgjNXHn15KOQgAvZ+1RGmMGbecc3EvIh6KiA0RsUHFMBtjBs9YDf9QRMwAgN7Pw6MNLKU8UkpZVUpZpXwfY8zgGWsAz48BPADgu72fT2Q2mjBhQiWwsaChPhXwHwwOxAFq4Y4rnpw8/qmobCwu3a0CLTgYRol0qi8T+MIZWSrwhvuUkMnnprK4uE8FxyjhLBNUw+JmRpRSc+TroebDfUoA5P2ozDcl7nEVHHXPWNxTWZ+8XJu6HjNnzuy0M8t8jVXsy3yd9yiAnwNYHBH7IuJBjBj8lyNiG4Av99rGmAuEvm/8Uso3RvmvO8/yXIwxA8KRe8Y0yECTdD7++OMqIEIlwTBceVctUcR+nvJ72Rfl4CGgDthQ/isH9Si/U/lnrDGo5ZR5iSYV6MHVhlQl4EzgC+sAmQSY7LjMGPbplZ6RWUY9cyz26ZUGpMgs68X3Qz17mf1y8tfBgwerMZkgsAx+4xvTIDZ8YxrEhm9Mg9jwjWmQgYp7x44dqyrssAimAiSuvfbaTluJaZklkng7tUQRCzMqY4znrDLxVBAHizXr16+vxrBQp5YU4zLhixcvrsaw4KREMRbX1LlmxCO1HfdlhLtMIJQSTfk8MhWBFEok5eOpake8nTo+349MRiEHBgHAK6+80mlnK+4wfuMb0yA2fGMaxIZvTIPY8I1pkIGKexFRiUycnXfPPfdU27EQwtuoMSoqjrOvMpFzqvQWCzxKcHrzzTerPhaq1PFZ8Hr11VerMc8++2ynzXMGgAULFnTaSgDkqMBM6Ss1R5VVxyXE1HXkPrUfjlR77733qjFcomrPnj3VmJ07d3baKhNPPVcsyilBmIVLFRXIc1RlujmKNVOea+/evZ12di09v/GNaRAbvjENYsM3pkHOewDP1772tU6bg1MAYN26dZ325MmTqzHsZylfh31z5YtxBR61rNHPf/7zTnvTpk3VGPYNgdrPUwEjPEeuygLU/qHyTTnwZ/v27dUY9p9VaTQVjMJaQKbkdabeotJFtmzZ0mmr+8H7VteDz2PRokXVGKXVcFlsVfKanz0VdMXagMoonDp1aqe9e/fuaszw8HCn/c4773Ta2aXb/MY3pkFs+MY0iA3fmAax4RvTIAMV9y677DKsWrWq07d06dJO+6c//Wm1HYsnLIKoMSpriccoIYRLb6kAGg4Q+exnP1uNuf3226u+NWvWdNpK8OIMPiUm8ZyWLVtWjeFgECWIsjCksuMygp8K/OHrqLLR+Pw3bNhQjWFxj+cM1ME4SjhjMU8FFGXLpPcbowKzduzY0WlnAm3Ufvi6zpkz57THGQ2/8Y1pEBu+MQ1iwzemQQbu4996662dvueee67TVr4P+7DKf89UXeHACuVDcZ/SE9hfVgEbHGgB1EkXqrQ4V+75+te/Xo158sknTzsfoPbNVVIKBxApf14FGfH1V4krXLqck0mAOtCGg7uA2u/npaiA+t6zHwwAzzzzTKetlmFTVZNuuOGGTltV4GGNQ1Vk4mdEaTf87Kn9sE7FeoaTdIwxo2LDN6ZBbPjGNIgN35gGGai4V0qpxBEWgVasWFFtx+KJCtDIBPCweKLEPQ7qUYLXbbfd1mlv3bq1GvPiiy9WfZxpp0Q5RgX5LF++vNNWpauVuMhwJR2VUagELw5YUdlwL730UqfN6/QBtdipgnP4minB68Ybb+y01Xk8+uijnbYKzFGiIM9bjeGMUrVmPVdJUmIni5QqyIiFVK5IlF3/0G98YxrEhm9Mg9jwjWmQgfr4H3zwQZV0wctjqao4hw4d6rSVb84+vfLN2V9V/iL7SFwtFqiXNlLroWeq7GYq+Ko10ufOndtpK3+e9QOlebB+oLQCpUPwvJVvztdN3bPMeXCQk7qvl19+eafNy6kBwJ133nna+QHA5s2bqz6GKzQBtVaRqVqkqgVz8I2qnsyaGN9D+/jGmFGx4RvTIDZ8Yxqkr+FHxOyIWBcRWyJic0R8q9d/VUSsiYhtvZ/1F77GmHFJRtw7DuAvSim/jIhPA3g+ItYA+GMAa0sp342IhwE8DODbp9tRRFTCC1cQ4fW/gTogQolQLF4pYYQFHbUfrhSjBEAWmFR23rRp06o+FsWUwMPijDp+RsxilJjFYqOqpKPgICe13cKFCzttJS6yKKgCX1auXNlpv/zyy9UYFtxYAAPqCjxKtNy3b1/VxyjxjMtyqzF8/ZUgzMt8qWXP+NpnxTym750upRwspfyy9/tRAFsAzAJwL4DVvWGrAdw3phkYYwbOJ/LxI2IugJsAPAtgeinlIDDyxwFA/Yob2eahiNgQERsyiyoYY849acOPiMsBPA7gz0opdRWBUSilPFJKWVVKWaW+gzXGDJ5UAE9ETMSI0f9TKeXfet2HImJGKeVgRMwAUDtWxKRJk3Ddddd1+tg/U8sYcWCD8s25b8qUKdUY9kXVftiHUpV42V9TVWqU388+rEpcyfhsvJ0KGOE5ZSrIKr9TwdspHYL1DKV58DU6cOBANYbnNH/+/GoMB0vxEuHqWOrTp0pIUs8jkwmEYv1ALZPNiU1qjmfr5ZlR9QPADwBsKaX83Sn/9WMAD/R+fwDAE2dlRsaYc07mjf9FAP8ZwEsRsbHX99cAvgvgnyPiQQB7ANTF4Ywx45K+hl9KeRpA/V3MCHeO0m+MGcc4cs+YBhlodt7Q0FDfdduVUMWimBrDARmZDCkleLGYp/aTXYOcYZFS7SezbxbuMuJaZimo7HmxeKWuEQcZKfg85s2bV43h50WVt+brqgKBVJUgRgVCcZCVOleuiqPOnTNMlUjJ26nzYHGP56wCpRR+4xvTIDZ8YxrEhm9MgwzUxz9x4kTle3KAhPJXOXFG+WKZirW8n4wOoAJqMj6lIpPcwsfLBBCpIBP2TTOBUWo+ymfk66gCkfg+qoAmnlNGK8hca+XPZxKSVCITBxCp4Bz28VUgFN9HXmodqAOI1D3jOXLFqqxO4ze+MQ1iwzemQWz4xjSIDd+YBhmouHf8+PEqk4oFHSW6sGChxBMWxTKiXEbMUmNYYMqsY67mpIQqFqFYSFN9Kjgns056Zj8qYIbFLCVC9VvHHaiXusoE5ygyoi3fI1XaXN0zFpLV88DipirBzfvhajsAsHTp0k5biY3cx6Xe1Tko/MY3pkFs+MY0iA3fmAax4RvTIAMV90opMsrrVFTkEa9NpgQWjlRTglumTDeLSSpyjY+fzYhi8UyJaTxHFaWYWYOPr3M2upBR4iIfTwlwfI3U8blPlTBjUVA9P5lz4/moslaq3DmjzpX71L75uVZz5nLjai1BFrY5ci9bIt1vfGMaxIZvTIPY8I1pkIFn57GPxv6RWkedgxRee+21agxXJlH+4liq6ygfl/1OFXii/Hfel/Lz+Hqo4Bj245Tfy/6qOg/WJpS+ktFTVHAOHy+z9JTSSjJz7LeNOr66ZyrwZurUqZ12RnPJVFZSwTkcGKVKabMupe5rBr/xjWkQG74xDWLDN6ZBbPjGNMhAxb2IqEQOFiumT59ebffGG2902kqY4cAGJYplMgG5Twk1vG8V1JEJ6skEtWTEm4y4qMQ1PlclOGVKoSky67hnsgMzYmtGNOXjHz16tBqj+ljcU88DH0+dR+Zaq7X7GN43zzkr9vmNb0yD2PCNaRAbvjENMlAfH6h9HU4y4Ao9QJ0UklnHXfm9mQSGTAls9unUsZRvzH6m0iHYh1NlqY8cOdJpc+AHUPuQyvfjY6nrqvxlviYqEIrPVSXA8BwzWonSCjiASZ0r75t1I0D75lzeW93XjP/Oc1LLbM2cObPvfrZt29ZpcynxbDKW3/jGNIgN35gGseEb0yA2fGMaZKDi3oQJE6qMIw5AeOutt6rtWOBS4g0LTCqAhkWgTAludSwWXVRQh6qKw6h9c3bilClTqjEslCnhjAVHJZzxeoNc7hrQQS1KdOp3fCUA8v1Q+81ko/GxlMDFVXGUuKcyOjmgTGXn8fVXGaYZ0ZjFVbUG4Ouvv95pv/zyy512vwpXvzt+apQx5v8rbPjGNEhfw4+ISRHxXES8EBGbI+Jve/3zIuLZiNgWET+KiPozkDFmXJLx8Y8BuKOU8puImAjg6Yj4KYA/B/D9UspjEfG/ADwI4B9Ot6MTJ05UPiMHTSg/K5M4M5ZqMsoXVEE1DPvvmXXugdpfVUkZ7EO+++671RjWSdS57t+/v9NWgVG33nprp81VXgGtH/C1VnNkf13dV/ZHVSAU70dpN3zPVEAT+8bqWEpPYX0pE9Ck7j0/9ypYis9N3TO+rxndStH3jV9GOHklJ/b+FQB3APjXXv9qAPeljmiMOe+kfPyIGIqIjQAOA1gD4DUAR0opJ//87QMw69xM0RhztkkZfinl41LKjQCGAXwewFI1TG0bEQ9FxIaI2KA+ghljBs8nUvVLKUcAPAXgFgBXRsRJjWAYwIFRtnmklLKqlLJKJSYYYwZPX3EvIqYC+KiUciQiLgVwF4DvAVgH4H4AjwF4AMAT/fY1YcKEStRQgpLa7lRUEAWLLkpw4yCSTHntjFii9qPELBaqMqWaVcAKb7d48eJqzLXXXttpc6AHAGzfvr3TVks/KbGTxU2Vscb3ORPQpI7PnxLVtWah7vDhw9UYDoZRzwdnigLA1Vdf3Wmr82CRVgl3vB3fH6AWSbds2VKN4QA3fqayS2hlVP0ZAFZHxBBGPiH8cynlJxHxMoDHIuK/AfgVgB+kjmiMOe/0NfxSyosAbhL9OzDi7xtjLjAcuWdMgww0Sefiiy/GnDlzOn2bN2/utJVPmVl+iP28TAKOGsO+ufIpM0tpZ6rSqHNlnz5T1VUt983nsXLlymrMpk2bOm0OchkNnhMn+wD1uWXuq9IKeFkr5WPzNVOBL3w/1L1XATzsrysdgivxZoKDVDXpDRs2dNr79u2rxvQLQlPJWAq/8Y1pEBu+MQ1iwzemQWz4xjTIQMW9UkolRnDVl4zApIIUWMzKrAevhCIOjlHRhhxokRGTgFoEU/vOBPlklozKlFm+5pprOm0lOCmxiIOuVAYfz1EJXpxZpgJoWPBTopxa157hOSqBePbs2VUfi3uZ7NFXX321GsPinprzCy+80Gkr0Zjnra59Br/xjWkQG74xDWLDN6ZBBu7js6/HPpMKkGB/Vfmv7IspX5ADRpTfycdXS1ixv6Z8deX379y5s9NWOgD72aryLV8zpQNkko34+Mp/VTpIJkiEA3YOHTpUjWGtZOnSOtub72tGK+AlxtSxhoeHqzGLFi2q+tjPVteaE4lUpdtp06Z12uvXr6/GcHUdrrQE9E/KySzPDviNb0yT2PCNaRAbvjENYsM3pkEGKu4dP368Clxg8UqtUc4VVTJrlCuhisUsJZyxMKQqBHE2lgo8Ucso8ZxeeeWVagyvf67Og/etglEywh33KQFQiXIsXCox68CBbiU2lWV48803d9qzZtX1WjlDTd0PvmdKWOVrdN1111VjuNoOUAeUqeCxHTt2dNpKAOT7oTLvWJhTGY1sH3wsi3vGmFGx4RvTIDZ8Yxpk4Mtks6/F1WNU9VGuBqt880ygBftZSitgf10FjHBQi/KrVJIQB/4sX768GsNVVFUwCvviKlGD96N800zCR6aSkdIBmBtuuKHqY59e+bS8nLXSgDJJOlz5acWKFdUYdc+YgwcPVn179uzptFW1Iw4gUsvBM+qesZ6Srapb7XtMWxljLmhs+MY0iA3fmAax4RvTIAMV94aGhirxjMUKrgoD1IIKiylALUIpcY/FPCWMZMopswilAmgyS2+poBrO9MtU1+GAIgCYOXNmp62CWlhwUmW6VQZhvyAsoA6GUfvh+6qCc1hwVOIez1vNh4OF1D1Ti7pyIJJanouDk3hJLUA/Rww/VyoLkp/rbDltxm98YxrEhm9Mg9jwjWkQG74xDTJQcS8iKkGLs59UuSEWqlT5Ys6QUuu5KcGPYaFIbcPimop4U1GBLAIpoYrFPRUVmMkgZJFUZb6pteIYXlceqKMZ1TXiPnWufN1Ulh+LpCoTkoXDZcuWVWP4/NWclWi8a9euTpufRaC+Rko45HNVAmQmCo/3zQKt184zxoyKDd+YBrHhG9MgA/fx2c/dvXt3pz1//vxqO/Z9rr/++mrM888/32lzRh9QZ8Mpf4iPpcb0q4IC6IozmeooPEZpBeznqawyDo5R82E9Q2XHqexA3i7jm6pAJFW6m+HsPBWIxBmdKuuR56yy47Zu3Vr1cUCXuteslajgLb5GvF+g1ipU0BNfR76GmcAxwG98Y5rEhm9Mg6QNPyKGIuJXEfGTXnteRDwbEdsi4kcR0f+7MmPMuOCTvPG/BWDLKe3vAfh+KWURgLcBPHg2J2aMOXekxL2IGAbwewD+O4A/jxEF6g4Af9gbshrAfwXwD590Aiyy8BpjQC2MKIHlS1/6Uqf9s5/9rBrD26kSyyyoZLLjVHCK2o6FOhXokSn/xMKQEuAyWW2MEjKVIMnnobL6WExUQU58jdR5cBlqtZbhggULOm21liGLYJx1B2gBlJ8ZJZ5xMI7aD6PW7uN5q/vBGYRZMY/JvvH/HsBfATh5lKsBHCmlnLxz+wDUoWHGmHFJX8OPiN8HcLiUcur3Zep7KRkrGBEPRcSGiNig/qIbYwZP5qP+FwHcExF3A5gE4AqMfAK4MiIu6r31hwHUn50AlFIeAfAIACxZsmRsVQOMMWeVvoZfSvkOgO8AQETcBuAvSyl/FBH/AuB+AI8BeADAE/32deLEiSoRg31PTjoA6mouyn+eMWNGp33XXXdVY5588sl+U8TixYs7beVzs++lSnCrAA1G+fiZYBj2c1WyzdGjRzttVV0m4x+qa806iJoz+/TqGrE2oIJz2O9dunRpNYYrECnfmO+jmrOqiMT+utKXMhoU991///3VGL6ujz/+eN/9qMSmDGfyPf63MSL0bceIz/+DM9iXMWaAfKKQ3VLKUwCe6v2+A8Dnz/6UjDHnGkfuGdMgNnxjGmSg2XnHjh3ruw6eEpxYAFQBI5xZpgKBvvKVr3Taa9eu7bsfDg4B6qAWJSaprLpMeW8Wb1SGFh9fVS1iwUuVd2ZhSAlwKvCH560EJv7qVu2b98MCLVA/DyqAh6+HEk25JLi69io7kSv+qICmTMlrFnvVc84CZKaaDouPmQxQwG98Y5rEhm9Mg9jwjWmQgfr477//PjZv3tzp4+qnykdhH1L5vewzKd+U/X72+QFg3bp1nbZKLlm4cGGnrQI2VOVbnrdK5uA+da6M0jzYP8xoBSpYSfWxL6zWp+d7tm3btmoML0d13333VWNee+21qo9RgTcMB4Ypf16dK99HNUZV3mVY39myZUs1hu+ZOi+urMTLdWUCwAC/8Y1pEhu+MQ1iwzemQWz4xjTIwMtrsxDFopMS5VjwUqIYZ5FlRCnO+gPqrL6nn366GrNx48ZOm8W+0cicBws86jw4YCcjiKogm0y1HwULXmqZLRZx1fJUn/vc5zrtzBryqrqOEjcZPn+VPakyEVksO3ToUDWGlytTAhv3qdoUfB/VufI1uuWWWzptFv9Gw298YxrEhm9Mg9jwjWmQgfr4Q0NDld8ylqWrVTAK+6vK72U/SyVKcNDEF77whWrMzp07O232ZwGdTLJkyZJOWyXy8HmoZabYz1NaAe9H+fMqcYbJHH///v3VGK6erHxqvvdKB+AkLhXUktF3GHXNMtWfVLASaxwqoIfnqBKr+JmdPn163zFz5szptNUzpfAb35gGseEb0yA2fGMaxIZvTIMMXNybPHlyp48DVpTYl6l4w2TGqEALFhJVNRdef50FFgBYv3591ffUU0912vPnz++7byUCcalsdR4sgCqxkwOaVMaayk5kEU6JYrNnz+60lQDIGXtKAOSMyoywqwJx+LnKZOIBueWwWMicO3du3zHqXPn6q3vWr7JSdkktv/GNaRAbvjENYsM3pkEG6uNPmDChCsBgX0v5tIxK5GGfPhPAk6mAo3xB9iFVsM7dd99d9XHgj0oAYr930aJF1RiuRpsJjlGaByeuKN947969VR/7lVxFSY1Ry0KzVqGChRg1R0Yl7WSqz6rryHNSGoPSBphMgBk/+ypYiIOD+HnJLqnlN74xDWLDN6ZBbPjGNIgN35gGiUygy1k7WMQbAHYDmAKgXgx9fHMhzhm4MOftOY+dOaWUqf0CKRS/AAADNUlEQVQGDdTwf3fQiA2llFUDP/AZcCHOGbgw5+05n3v8Ud+YBrHhG9Mg58vwHzlPxz0TLsQ5AxfmvD3nc8x58fGNMecXf9Q3pkEGbvgR8dWIeDUitkfEw4M+foaI+GFEHI6ITaf0XRURayJiW+/nZ063j0ETEbMjYl1EbImIzRHxrV7/uJ13REyKiOci4oXenP+21z8vIp7tzflHEdG/IuuAiYihiPhVRPyk1x73cz6VgRp+RAwB+J8AvgZgGYBvRMSyQc4hyT8C+Cr1PQxgbSllEYC1vfZ44jiAvyilLAVwC4A/6V3b8TzvYwDuKKWsBHAjgK9GxC0Avgfg+705vw3gwfM4x9H4FoBT17q+EOb8Owb9xv88gO2llB2llA8BPAbg3gHPoS+llP8A8BZ13wtgde/31QDqxdzPI6WUg6WUX/Z+P4qRh3IWxvG8ywgnU/Qm9v4VAHcA+Nde/7iaMwBExDCA3wPwv3vtwDifMzNow58F4NQ8z329vguB6aWUg8CIkQGY1mf8eSMi5gK4CcCzGOfz7n1k3gjgMIA1AF4DcKSUcjL/djw+I38P4K8AnKxzdTXG/5w7DNrwVVK0v1Y4i0TE5QAeB/BnpZS6GN44o5TycSnlRgDDGPlEuFQNG+ysRicifh/A4VLK86d2i6HjZs6KgRbiwMhfwlOrMA4DODDgOYyVQxExo5RyMCJmYOQNNa6IiIkYMfp/KqX8W6973M8bAEopRyLiKYzoE1dGxEW9N+h4e0a+COCeiLgbwCQAV2DkE8B4nnPFoN/46wEs6imgFwP4AwA/HvAcxsqPATzQ+/0BAE+cx7lU9PzMHwDYUkr5u1P+a9zOOyKmRsSVvd8vBXAXRrSJdQDu7w0bV3MupXynlDJcSpmLkef3yVLKH2Ecz1lSShnoPwB3A9iKEV/ubwZ9/OQcHwVwEMBHGPmU8iBG/Li1ALb1fl51vudJc/5PGPl4+SKAjb1/d4/neQNYAeBXvTlvAvBfev3zATwHYDuAfwFwyfme6yjzvw3ATy6kOZ/858g9YxrEkXvGNIgN35gGseEb0yA2fGMaxIZvTIPY8I1pEBu+MQ1iwzemQf4f2fn8XdZJzIQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[20000].reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
    "plt.show()\n",
    "print(y_train[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Reshape\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D,  BatchNormalization,  Activation,ZeroPadding2D\n",
    "from keras import optimizers, callbacks, layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_classes = y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3),  padding='same',activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#model.load_weights('my_model_bestwts_mod.h5')\n",
    "sgd = optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam=optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls=callbacks.ModelCheckpoint(filepath=\"my_model_bestwts_mod.h5\",monitor='val_acc',\n",
    " #                                     save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 1,283,623\n",
      "Trainable params: 1,283,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model.fit(train_X, train_y, batch_size=64, epochs=15, verbose=1, validation_data=(test_X, test_y),callbacks=[calls])\n",
    "#model.save_weights('my_model_weights_mod.h5')\n",
    "datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=10,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.25,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=False,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "maxepoches = 60\n",
    "learning_rate = 0.2\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "28/28 [==============================] - 124s 4s/step - loss: 12.7855 - acc: 0.1631 - val_loss: 13.4504 - val_acc: 0.1655\n",
      "Epoch 2/60\n",
      "16/28 [================>.............] - ETA: 52s - loss: 13.4504 - acc: 0.1655"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-d77b57649213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                   verbose=1)\n\u001b[0m\u001b[1;32m      9\u001b[0m '''\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#using image augmentation\n",
    "historytemp = model.fit_generator(datagen.flow(X_train, y_train,\n",
    "                             batch_size=batch_size),\n",
    "                steps_per_epoch=X_train.shape[0] //batch_size,\n",
    "                epochs=maxepoches,\n",
    "                validation_data=(X_val, y_val),\n",
    "                                  callbacks=[reduce_lr],\n",
    "                                  verbose=1)\n",
    "'''\n",
    "model.fit(X_train, y_train, batch_size=100, epochs=60, verbose=1, validation_data=(X_val, y_val))\n",
    "         # ,callbacks=[calls])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/60\n",
      "22500/28709 [======================>.......] - ETA: 28s - loss: 13.4568 - acc: 0.1651"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-ff58dd0c5c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0;31m# ,callbacks=[calls])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#no image agumentation\n",
    "model.fit(X_train, y_train, batch_size=100, epochs=60, verbose=1, validation_data=(X_val, y_val))\n",
    "         # ,callbacks=[calls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
